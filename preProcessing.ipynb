{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, zoom_range=0.3, horizontal_flip=True)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "TRAIN_DATA_PATH = '/Users/kaylienguyen/Downloads/FER_dataset/trainrevised'\n",
    "TEST_DATA_PATH = '/Users/kaylienguyen/Downloads/FER_dataset/testrevised'\n",
    "IMAGE_SIZE = (48, 48)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28614 images belonging to 7 classes.\n",
      "Found 7071 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (48, 48, 1)\n",
    "epochs = 10\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=(48, 48, 3))\n",
    "conv_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters=10, kernel_size=3, strides=(1, 1), padding='valid', activation='relu', input_shape=input_shape, kernel_regularizer=l2(.01)),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=20, kernel_size=3, strides=(1, 1), padding='valid', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    BatchNormalization(),\n",
    "    #Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid', activation='relu'),\n",
    "    #Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid', activation='relu'),\n",
    "    #Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid', activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation = 'relu'),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "448/448 [==============================] - 26s 57ms/step - loss: 1.7717 - accuracy: 0.3007 - val_loss: 1.7137 - val_accuracy: 0.3007 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "448/448 [==============================] - 24s 53ms/step - loss: 1.6011 - accuracy: 0.3794 - val_loss: 1.5203 - val_accuracy: 0.4021 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.5276 - accuracy: 0.4101 - val_loss: 1.4385 - val_accuracy: 0.4477 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.4720 - accuracy: 0.4358 - val_loss: 1.4251 - val_accuracy: 0.4533 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.4268 - accuracy: 0.4560 - val_loss: 1.3534 - val_accuracy: 0.4830 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "448/448 [==============================] - 24s 53ms/step - loss: 1.3867 - accuracy: 0.4735 - val_loss: 1.3053 - val_accuracy: 0.4996 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "448/448 [==============================] - 24s 54ms/step - loss: 1.3520 - accuracy: 0.4844 - val_loss: 1.2868 - val_accuracy: 0.5076 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "448/448 [==============================] - 26s 58ms/step - loss: 1.3213 - accuracy: 0.5036 - val_loss: 1.3727 - val_accuracy: 0.4694 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.2910 - accuracy: 0.5103 - val_loss: 1.2847 - val_accuracy: 0.5134 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "448/448 [==============================] - 27s 60ms/step - loss: 1.2685 - accuracy: 0.5212 - val_loss: 1.3824 - val_accuracy: 0.4698 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/hxyrmk3x0fndxrfylccz8ldh0000gn/T/ipykernel_98249/2991971360.py:37: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  test_loss, test_accuracy = model.evaluate_generator(test_generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  1.3824172019958496\n",
      "Test Accuracy: 0.4698062539100647\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "initial_learning_rate = 1e-4\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',   \n",
    "    factor=0.2,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=3,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=initial_learning_rate),  \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    validation_data = test_generator,\n",
    "    callbacks=[lr_scheduler]  \n",
    ")\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizers.Adam(lr=1e-4))\n",
    "#model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate_generator(test_generator)\n",
    "print('Test Loss: ', test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
